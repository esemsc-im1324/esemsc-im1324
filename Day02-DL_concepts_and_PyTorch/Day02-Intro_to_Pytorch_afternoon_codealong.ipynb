{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/esemsc-im1324/esemsc-im1324/blob/main/Day02-DL_concepts_and_PyTorch/Day02-Intro_to_Pytorch_afternoon_codealong.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCdj-_pnBqhA"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1YLNtm8gNsviTEnVXzfiby2VMKrc0XzLP\" width=\"500\"/>\n",
        "\n",
        "---\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://pytorch.org/assets/images/pytorch-logo.png\" alt=\"drawing\" width=\"100\"/>\n",
        "</p>\n",
        "\n",
        "\n",
        "\n",
        "<h1 style=\"text-align: center;\"> Introduction to Pytorch for Deep Learning</h1>\n",
        "\n",
        "\n",
        "\n",
        "In this introduction to PyTorch for deep learning, we will dive into certain packages and utilities of the library made to facilitate data handling and training opertations. We will explore similar concepts that were seen in the previous lectures, but in a different way. We will specifically focus on the functions of ``Dataset`` and ``Dataloader`` objects and their roles, also exploring adjacent functionlities for data splitting, pre-processing, and analysis. We will also discuss the basic training workflow and how ``PyTorch`` integrates these different steps. We must consider that ``PyTorch`` is an extensive library, and these lectures are designed to introduce you to its main concepts, particularly the ones that are useful for the completion of this module. With the links and references below, we encourage you to further explore the library as you see fit.\n",
        "\n",
        "\n",
        "#### **Morning contents/agenda**\n",
        "1. Basic functionalities: Tensor Handling like Numpy Arrays\n",
        "\n",
        "2. Data Handling with ``Torchvision``\n",
        "\n",
        "3. Batch Handling with ``Dataloader``\n",
        "\n",
        "4. Review of the ``Pytorch`` training workflow for a classification task\n",
        "\n",
        "5. Customisation of ``Dataset`` classes\n",
        "\n",
        "\n",
        "#### **Afternoon contents/agenda**\n",
        "\n",
        "1. Understanding the basics:\n",
        "- [But what is a convolution?](https://www.youtube.com/watch?v=KuXjwB4LzSA&ab_channel=3Blue1Brown)\n",
        "\n",
        "- [But what is a neural network?](https://www.youtube.com/watch?v=aircAruvnKk&t=1s&ab_channel=3Blue1Brown)\n",
        "\n",
        "- [What is backpropagation really doing?](https://www.youtube.com/watch?v=Ilg3gGewQ5U&t=2s&ab_channel=3Blue1Brown)\n",
        "\n",
        "2. Exercise 2\n",
        "\n",
        "#### **Learning Outcomes**\n",
        "\n",
        "1. Get acquainted with common ``Torchvision`` datasets and integrate them into a ``Pytorch`` workflow\n",
        "\n",
        "2. Learn the basic usage of ``torchvision.transforms``\n",
        "\n",
        "3. Understand the difference between a ``Dataset`` and a ``Dataloader`` and how to integrate them\n",
        "\n",
        "4. Understand the ``Pytorch`` workflow for training a classification model\n",
        "\n",
        "5. Learn to create custom ``Dataset`` classes\n",
        "\n",
        "7. Learn to access the ``Pytorch`` documentation\n",
        "\n",
        "<br/>\n",
        "\n",
        "---\n",
        "\n",
        "<br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "30KsuqBPBzld",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ac450ad-f1b8-4a15-dc35-d94d870a4721"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "except:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pBG1w14BBqhC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "290ab306-4058-4f0d-c99c-657aca1b4a2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No GPU available! Running on CPU\n"
          ]
        }
      ],
      "source": [
        "!pip install torch-summary progressbar2 -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = True  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
        "    torch.backends.cudnn.enabled   = True\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def set_device(device=\"cpu\", idx=0):\n",
        "    if device != \"cpu\":\n",
        "        if torch.cuda.device_count() > idx and torch.cuda.is_available():\n",
        "            print(\"Cuda installed! Running on GPU {} {}!\".format(idx, torch.cuda.get_device_name(idx)))\n",
        "            device=\"cuda:{}\".format(idx)\n",
        "        elif torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
        "            print(\"Cuda installed but only {} GPU(s) available! Running on GPU 0 {}!\".format(torch.cuda.device_count(), torch.cuda.get_device_name()))\n",
        "            device=\"cuda:0\"\n",
        "        else:\n",
        "            device=\"cpu\"\n",
        "            print(\"No GPU available! Running on CPU\")\n",
        "    return device\n",
        "\n",
        "device = set_device(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recap of Classes in Python"
      ],
      "metadata": {
        "id": "rR1NmjeisNz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Class and objects\n",
        "###\n",
        "class MyClass:\n",
        "  def __init__(self):           #This is the simple class, it contains nothing\n",
        "      pass\n",
        "\n",
        "\n",
        "my_object=MyClass()             #this  object does nothing, because the class contains nothing\n",
        "\n",
        "###\n",
        "\n",
        "# # Methods and Attributes (functions and variables of the class object)\n",
        "# class MyClass:\n",
        "#     def __init__(self):               #Methods are functions that are very particular to the class that we are creasting. And variables and\n",
        "#         ###\n",
        "#         return\n",
        "\n",
        "#     def my_method(self):\n",
        "#         return ###\n",
        "\n",
        "###\n",
        "###\n",
        "###"
      ],
      "metadata": {
        "id": "B_Nleqp8vNtJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Magic methods in Python are special methods that enable customisation of the behaviour of built-in operations in Python classes, such as arithmetic operations, comparisons, and object creation. Examples include ``__init__`` for initialising objects, ``__str__`` for defining string representation, and ``__add__`` for implementing addition."
      ],
      "metadata": {
        "id": "TCYOrhYYC8Ek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Magic methods\n",
        "class MyClass:\n",
        "    def __init__(self, N): # What the object does when it is first created\n",
        "        print(\"__init__\")\n",
        "        self.my_attribute=[i for i in range(N)]\n",
        "        return\n",
        "\n",
        "    def __str__(self): # What the object returns when it is cast into a string object\n",
        "        print(\"\\n __str__\")\n",
        "        return \"My custom Class\" ###\n",
        "\n",
        "    def __call__(self, x): # What the object does when it is called as a function\n",
        "        print(\"\\n __call__\")\n",
        "        return x**2\n",
        "\n",
        "    def __len__(self): # What the object returns when it's length is queried\n",
        "        print(\"\\n __len__\")\n",
        "        return len(self.my_attribute)\n",
        "\n",
        "    def __getitem__(self, key): # What the object returns when it is indexed\n",
        "        print(\"\\n __getitem__\")\n",
        "        return self.my_attribute[key]\n",
        "\n",
        "    def __setitem__(self, key, value): # How the object is modefied after index assignement\n",
        "        print(\"\\n __setitem__\")\n",
        "        self.my_attribute[key]=value\n",
        "        return\n",
        "\n",
        "# How to use each magic method:\n",
        "my_object = MyClass(10) # __init__\n",
        "print(my_object.my_attribute)\n",
        "\n",
        "my_other_object = MyClass(5)\n",
        "print(my_other_object.my_attribute)\n",
        "###\n",
        "\n",
        "print(my_object)### # __str__\n",
        "\n",
        "print(my_object(x=7))### # __call__\n",
        "\n",
        "print(len(my_object))### # __len__\n",
        "\n",
        "### # __getitem__\n",
        "print(my_object[-1])\n",
        "\n",
        "### # __setitem__\n",
        "my_object[-1]=-2.7864\n",
        "\n",
        "\n",
        "### # __getitem__\n",
        "print(my_object[-1])\n",
        "\n",
        "print(\" \\n ------ \\n\")\n",
        "print(my_object.my_attribute)"
      ],
      "metadata": {
        "id": "IdFoI0n4sBHF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "810c5aaf-5c02-4601-d42f-48cc92bc12b0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__init__\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "__init__\n",
            "[0, 1, 2, 3, 4]\n",
            "\n",
            " __str__\n",
            "My custom Class\n",
            "\n",
            " __call__\n",
            "49\n",
            "\n",
            " __len__\n",
            "10\n",
            "\n",
            " __getitem__\n",
            "9\n",
            "\n",
            " __setitem__\n",
            "\n",
            " __getitem__\n",
            "-2.7864\n",
            " \n",
            " ------ \n",
            "\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, -2.7864]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Python, ``self`` is a reference to the instance of the class, allowing access to its attributes and methods"
      ],
      "metadata": {
        "id": "Agr7OyKiJcHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Magic methods\n",
        "class MyClass:\n",
        "    def __init__(self): # What the object does when it is first created\n",
        "        ###\n",
        "        return\n",
        "\n",
        "    def __call__(self, x): # What the object does when it is called as a function\n",
        "        return x**2 ###\n",
        "\n",
        "\n",
        "my_object = MyClass()\n",
        "my_object(7)"
      ],
      "metadata": {
        "id": "L6lcC2HyJSwI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f226dc7e-da8c-4c5e-b8ff-3ac8fa9407de"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "49"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An important concept in OOD is *Inheritance*.\n",
        "\n",
        "Classes can inherit code and attributes from other classes to reuse shared logic while creating a distinct hierarchy."
      ],
      "metadata": {
        "id": "B-8Gl_ksKd_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DUtbC_d0FAd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parent class\n",
        "class Animal:\n",
        "    def __init__(self, name, age):\n",
        "        print(\"Creates an animal\")\n",
        "        self.name = name\n",
        "        self.age = age\n",
        "        self.kind = None\n",
        "        return\n",
        "\n",
        "    def speak(self): # Each inherited class is responsible for implementing its own `speak` method\n",
        "        raise NotImplementedError(\"This is an abstract method, and needs implementation from derived classes\")\n",
        "\n",
        "dog=Animal(name=\"Max\", age=2)\n",
        "print(dog.name, dog.age,dog.kind)\n",
        "print(dog.speak())\n",
        "print(\"---\")\n"
      ],
      "metadata": {
        "id": "ZvELaduEUz9Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "outputId": "a19b7c5f-edc3-4508-e52e-2dc399f4593d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creates an animal\n",
            "Max 2 None\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "This is an abstract method, and needs implementation from derived classes",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-823ae03ab981>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAnimal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Max\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspeak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-823ae03ab981>\u001b[0m in \u001b[0;36mspeak\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mspeak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Each inherited class is responsible for implementing its own `speak` method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"This is an abstract method, and needs implementation from derived classes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAnimal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Max\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: This is an abstract method, and needs implementation from derived classes"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Child class\n",
        "class Dog(Animal):\n",
        "    def __init__(self, name, age, coat):\n",
        "        print(\"Creates a dog\")\n",
        "        super().__init__(name,age)\n",
        "        self.coat = coat\n",
        "        self.kind = \"dog\"\n",
        "        self.breed = None\n",
        "        return\n",
        "\n",
        "    def speak(self):\n",
        "        return \"Dog: Woof!\"\n",
        "\n",
        "\n",
        "dog=Dog(name=\"max\", age=2, coat=\"brown\")            #This is an object\n",
        "\n",
        "print(dog.name, dog.age, dog.coat, dog.kind)\n",
        "print(dog.speak())\n",
        "###\n",
        "print(\"---\")\n"
      ],
      "metadata": {
        "id": "U-HQCtJxU1M5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3112618e-f5b1-429a-d2fd-b408b83130c0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creates a dog\n",
            "Creates an animal\n",
            "max 2 brown dog\n",
            "Dog: Woof!\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Child class\n",
        "class Bird(Animal):\n",
        "    def __init__(self, name, age, wingspan=10):\n",
        "        print(\"Creates a bird\")\n",
        "        super().__init__(name, age)\n",
        "        self.kind = \"bird\"\n",
        "        self.wingspan = 10\n",
        "        return\n",
        "\n",
        "    def speak(self):\n",
        "        return \"Chirp\"\n",
        "\n",
        "bird = Bird(name=\"Tweety\", age=1, wingspan=10)\n",
        "print(bird.name, bird.age, bird.wingspan, bird.kind, bird.speak())\n",
        "print(\"---\")"
      ],
      "metadata": {
        "id": "0cUMHSVyU3UG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5a4532d-53a8-4327-a24a-869a31f962a0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creates a bird\n",
            "Creates an animal\n",
            "Tweety 1 10 bird Chirp\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Grandchild class\n",
        "class Boxer(Dog):\n",
        "    def __init__(self, name, age, **kwargs):        #Buradakı KeywordArguments Dog klasında olan digər argumentləri bura kopy edir asanlıqla. Bu halda \"coat\"-ı kopyalayacaq.\n",
        "        print(\"Creates a boxer\")\n",
        "        super().__init__(name, age, **kwargs)\n",
        "        self.breed = \"Boxer\"\n",
        "        return\n",
        "\n",
        "    def speak(self):\n",
        "        return \"Loud Woof\"\n",
        "\n",
        "dog = Boxer(name=\"Cherry\", age=2, coat=\"brown\")\n",
        "print(dog.name, dog.age, dog.breed, dog.coat, dog.kind, dog.speak())\n",
        "print(\"---\")"
      ],
      "metadata": {
        "id": "9mlEvXA8wIoJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ec6e20e-4aa7-4108-89be-08e8cef18c4a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creates a boxer\n",
            "Creates a dog\n",
            "Creates an animal\n",
            "Cherry 2 Boxer brown dog Loud Woof\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why and When to Use Classes? (Object-Oriented Programming)\n",
        "\n",
        "- **Organisation:** Classes provide a structured way to group related data and behaviours, making code more intuitive and easier to follow.\n",
        "\n",
        "- **Modularity:** Through encapsulation, classes create self-contained objects, simplifying debugging and enhancing collaboration in team projects.\n",
        "\n",
        "- **Reusability and Productivity:** By leveraging inheritance, existing code can be reused and extended, reducing duplication and saving development time.\n",
        "\n",
        "- **Scalability and Maintainability:** Classes allow functionalities to be developed and upgraded independently, making them particularly advantageous for systems that are dynamic or require frequent updates.\n",
        "\n",
        "\n",
        "PyTorch is predominantly object-oriented for model organisation and design. Let's have a look at our model class from the previous lectures"
      ],
      "metadata": {
        "id": "qBUItVP0FpiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Child class inherited from nn.Module\n",
        "class simpleFFN(nn.Module):\n",
        "\n",
        "  # Initialise with a few parameters that will define our model\n",
        "  def __init__(self, input_size, hidden_size_1=100, hidden_size_2=50, output_size=10):\n",
        "\n",
        "    # Ensure parent is correctly initialised (avoids errors)\n",
        "    super(simpleFFN, self).__init__()\n",
        "\n",
        "    # Attributes are layers and activation of our model\n",
        "    # Note they are also instantiated objects of a class\n",
        "    self.hidden_1 = nn.Linear(input_size, hidden_size_1, bias=False)\n",
        "    self.hidden_2 = nn.Linear(hidden_size_1, hidden_size_2, bias=False)\n",
        "    self.output = nn.Linear(hidden_size_2, output_size, bias=False)\n",
        "    self.activation = nn.Sigmoid()\n",
        "\n",
        "  # 'forward' is a required method of nn.Module derived classes\n",
        "  def forward(self, X):\n",
        "    print(\"Running model forward pass\")\n",
        "\n",
        "    # Cascade the variable X through the model calling each layer instance\n",
        "    z1 = self.hidden_1(X.flatten(start_dim=1))\n",
        "    a1 = self.activation(z1)\n",
        "    z2 = self.hidden_2(a1)\n",
        "    a2 = self.activation(z2)\n",
        "    z3 = self.output(a2)\n",
        "    a3 = self.activation(z3)\n",
        "    return a3\n",
        "\n",
        "model = simpleFFN(input_size=1*28*28, hidden_size_1=200, hidden_size_2=50, output_size=10)\n",
        "print(model) #  __str__\n",
        "\n",
        "# The 'forward' function functions as __call__\n",
        "print(model(torch.rand(1,1,28,28))) # __call__"
      ],
      "metadata": {
        "id": "qD-HBV8jyKpO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c16fd87-05a3-4e50-ad19-9eec2381014f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "simpleFFN(\n",
            "  (hidden_1): Linear(in_features=784, out_features=200, bias=False)\n",
            "  (hidden_2): Linear(in_features=200, out_features=50, bias=False)\n",
            "  (output): Linear(in_features=50, out_features=10, bias=False)\n",
            "  (activation): Sigmoid()\n",
            ")\n",
            "Running model forward pass\n",
            "tensor([[0.5172, 0.4826, 0.4391, 0.4806, 0.3698, 0.4793, 0.4213, 0.6092, 0.4250,\n",
            "         0.4942]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic functionalities: Tensor Handling like Numpy Arrays"
      ],
      "metadata": {
        "id": "Y6smNB4gIgEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_data=[i for i in range(100)]\n",
        "\n",
        "# my_array=np.array(my_data)            #Pytorch bir qeder Numpy kitabxanasına oxşayır. Burada numpyla oxşarlığını müəllim qeyd etdi.\n",
        "my_tensor=torch.tensor(my_data)\n",
        "\n",
        "my_tensor, my_tensor.shape            #Bu 1D tensor, 100 elementi olan bir sıra kimi davranır"
      ],
      "metadata": {
        "id": "WUoHynl7Igl0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b076fb50-3c3e-40a9-ba9d-ba881ea0930a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
              "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
              "         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
              "         54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n",
              "         72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
              "         90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
              " torch.Size([100]))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_tensor=my_tensor.unsqueeze(1).unsqueeze(-1)\n",
        "my_tensor.shape\n",
        "\n",
        "\n",
        "#BatchSize, NumberOfChannels, Height, Width"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgFIgboJEafz",
        "outputId": "2b4108e8-112f-45bb-eac6-5495372939a4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_reshaped_tensor=my_tensor.view(1,25,4)               #1-ci Batc Numberdir, 2-inci raw-ların sayıdır, 3-cü Column-ların sayıdır.\n",
        "my_reshaped_tensor, my_reshaped_tensor.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36K3l3Z-FDsq",
        "outputId": "22e5d416-3b66-4edc-bf49-09f01bee13bf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 0,  1,  2,  3],\n",
              "          [ 4,  5,  6,  7],\n",
              "          [ 8,  9, 10, 11],\n",
              "          [12, 13, 14, 15],\n",
              "          [16, 17, 18, 19],\n",
              "          [20, 21, 22, 23],\n",
              "          [24, 25, 26, 27],\n",
              "          [28, 29, 30, 31],\n",
              "          [32, 33, 34, 35],\n",
              "          [36, 37, 38, 39],\n",
              "          [40, 41, 42, 43],\n",
              "          [44, 45, 46, 47],\n",
              "          [48, 49, 50, 51],\n",
              "          [52, 53, 54, 55],\n",
              "          [56, 57, 58, 59],\n",
              "          [60, 61, 62, 63],\n",
              "          [64, 65, 66, 67],\n",
              "          [68, 69, 70, 71],\n",
              "          [72, 73, 74, 75],\n",
              "          [76, 77, 78, 79],\n",
              "          [80, 81, 82, 83],\n",
              "          [84, 85, 86, 87],\n",
              "          [88, 89, 90, 91],\n",
              "          [92, 93, 94, 95],\n",
              "          [96, 97, 98, 99]]]),\n",
              " torch.Size([1, 25, 4]))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_reshaped_tensor=my_tensor.view(1,5,20)               #1-ci Batc Numberdir, 2-inci raw-ların sayıdır, 3-cü Column-ların sayıdır.\n",
        "my_reshaped_tensor, my_reshaped_tensor.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEnLIn-cF7OC",
        "outputId": "cc1d702c-ce3b-4e7f-9564-6ceab743affb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
              "           17, 18, 19],\n",
              "          [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
              "           37, 38, 39],\n",
              "          [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56,\n",
              "           57, 58, 59],\n",
              "          [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76,\n",
              "           77, 78, 79],\n",
              "          [80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96,\n",
              "           97, 98, 99]]]),\n",
              " torch.Size([1, 5, 20]))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rand_tensor=torch.rand((3,256,256))+1000              #Sen burada tensor yaratdın. Əgər istəsən həmin tensorun üzərində operator işləri apara bilərsən\n",
        "# rand_tensor\n",
        "rand_tensor.mean(), rand_tensor.std(), rand_tensor.max(), rand_tensor.min()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYpovzkIIBT_",
        "outputId": "7dde8292-9199-4ce1-a66f-6f6a3d56108c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(1000.5007), tensor(0.2882), tensor(1001.), tensor(1000.))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aşağıdakı kodlar menim öz yazdığım nümunə kodlardır."
      ],
      "metadata": {
        "id": "zLgQCJ-RC35D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mənim nümunə kodum\n",
        "my_tensor=my_tensor.unsqueeze(-1)\n",
        "my_tensor.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aLm--l11Wtk",
        "outputId": "63aa019c-6f9e-461b-e149-e98be4038e03"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mənim nümunə kodum\n",
        "my_tensor=my_tensor.unsqueeze(0).unsqueeze(-1)\n",
        "my_tensor.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UF1PUzWY0qV1",
        "outputId": "bd2749b9-0d69-45fe-973d-e0474dc4ee64"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 100, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mənim nümunə kodum\n",
        "tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "print(tensor.shape)  # Çıxış: torch.Size([2, 3])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdH9IxiQ2g8N",
        "outputId": "53624ce9-d4ff-4f3e-aaa5-e4687c282004"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mənim nümunə kodum\n",
        "# 2-ci ölçüdə yeni bir dimension əlavə etmək\n",
        "tensor = tensor.unsqueeze(0)\n",
        "print(tensor.shape)  # Çıxış: torch.Size([2, 3, 1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lsczhar42jHT",
        "outputId": "98245367-f045-4248-e93d-cdc4cd6bf740"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVSEqiLtBqhD"
      },
      "source": [
        "## Data Handling with Torchvision's KMNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN_erM55BqhD"
      },
      "source": [
        "[``Torchvision``](https://pytorch.org/vision/stable/index.html) is one of many support libraries for the ``Pytorch`` project. It provides a range of popular models (pre-trained or not) and datasets, as well as image transformations that are useful for computer vision problems. Because it is a support library, its integration to ``Pytorch`` is native and straigth-forward, as we will see below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "F8Wd0rWcBqhD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca6c9854-4676-454e-ff3d-5b3129d6ffd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-images-idx3-ubyte.gz to ./KMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 18.2M/18.2M [00:12<00:00, 1.49MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./KMNIST/raw/train-images-idx3-ubyte.gz to ./KMNIST/raw\n",
            "\n",
            "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-labels-idx1-ubyte.gz to ./KMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 202kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./KMNIST/raw/train-labels-idx1-ubyte.gz to ./KMNIST/raw\n",
            "\n",
            "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-images-idx3-ubyte.gz to ./KMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3.04M/3.04M [00:02<00:00, 1.09MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./KMNIST/raw/t10k-images-idx3-ubyte.gz to ./KMNIST/raw\n",
            "\n",
            "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-labels-idx1-ubyte.gz to ./KMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.12k/5.12k [00:00<00:00, 15.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./KMNIST/raw/t10k-labels-idx1-ubyte.gz to ./KMNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "\n",
        "train_dataset = torchvision.datasets.KMNIST(root=\"./\",train=True, download=True)\n",
        "test_dataset = torchvision.datasets.KMNIST(root=\"./\",train=False, download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "GkfYNzSPBqhD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8901a015-c1c2-49d0-bad9-720ab14f155e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torchvision.datasets.mnist.KMNIST'> \n",
            "\n",
            "Dataset KMNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: ./\n",
            "    Split: Train \n",
            "\n",
            "['__add__', '__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_check_exists', '_check_legacy_exist', '_format_transform_repr', '_is_protocol', '_load_data', '_load_legacy_data', '_repr_indent', 'class_to_idx', 'classes', 'data', 'download', 'extra_repr', 'mirrors', 'processed_folder', 'raw_folder', 'resources', 'root', 'target_transform', 'targets', 'test_data', 'test_file', 'test_labels', 'train', 'train_data', 'train_labels', 'training_file', 'transform', 'transforms'] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(type(train_dataset), \"\\n\")\n",
        "print(train_dataset, \"\\n\")\n",
        "print(dir(train_dataset), \"\\n\") # Information held in the dataset object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "mCmYlWdIBqhE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7442b51e-768b-456e-fe74-5f673bf4267c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'o': 0, 'ki': 1, 'su': 2, 'tsu': 3, 'na': 4, 'ha': 5, 'ma': 6, 'ya': 7, 're': 8, 'wo': 9}\n"
          ]
        }
      ],
      "source": [
        "class_to_idx = train_dataset.class_to_idx\n",
        "print(class_to_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Xv-4YFGBqhE"
      },
      "source": [
        "* The method ``__getitem__`` is called when indexing samples within Pytorch Datasets\n",
        "\n",
        "* For this dataset class, ``__getitem__`` returns two variables: the image and its target label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Yzq2kDYxBqhE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "88d580db-54b4-4423-95f1-be722198fc7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<PIL.Image.Image image mode=L size=28x28 at 0x7929367DB670>, 8) \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "sample =  train_dataset[0]\n",
        "print(sample, \"\\n\")\n",
        "\n",
        "plt.imshow(sample[0], cmap=\"gray\")\n",
        "plt.title(list(class_to_idx)[sample[1]])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vz5-a9YtBqhE"
      },
      "source": [
        "#### Let's visualise a few samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32TUHGojBqhE"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "idxs = torch.randint(low=0, high=len(train_dataset), size=(6,))\n",
        "fig, axs = plt.subplots(1, 6, figsize=(15, 5))\n",
        "for i, idx in enumerate((idxs)):\n",
        "    img, target = ###\n",
        "    axs[i].imshow(###)\n",
        "    axs[i].set_title(str(target) + \" / \" + class_to_idx[target])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0-WgUldBqhE"
      },
      "source": [
        "* For convenience, let's change the format of the image from ``PIL`` to a ``torch.tensor`` object\n",
        "\n",
        "* We will do so by using the ``torchvision.transforms`` module\n",
        "\n",
        "* A list of transforms is often packaged into a ``Compose`` container\n",
        "\n",
        "* Pytorch's standard practice is to apply such transformations inside the method ``__getitem__``\n",
        "\n",
        "* It doesn't modify the data itself, but it will be applied whenever the dataset is indexed\n",
        "\n",
        "\n",
        "* This circumvents the need of using pre-processing scripts to generate and store a processed dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mE0q4fnIBqhE"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import Compose, ToTensor\n",
        "transform = Compose([ToTensor(),]) # Compose a list of transformations\n",
        "\n",
        "train_dataset = ###\n",
        "test_dataset = ###\n",
        "\n",
        "sample = ##\n",
        "img, target = sample\n",
        "\n",
        "print(img.shape, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3JgrKjEBqhF"
      },
      "source": [
        "* Using a ``ToTensor`` transformation is equivalent to calling ``TensorDataset``, as seen in the previous lecture. However, ``TensorDataset`` requires all data to be loaded into memory, which is not often feasible. Using the transformation allows the data to be transformed into ``torch.tensor`` objects as the data is accessed.\n",
        "\n",
        "* Other transformations from the ``torch.transforms`` module can be found in its documentation [here](https://pytorch.org/vision/0.9/transforms.html)\n",
        "\n",
        "* These include ``Normalize``, ``Pad``, ``RandomCrop``, ``Resize``, etc..\n",
        "\n",
        "* There is even an option to create custom transforms with ``transforms.Lambda``. Let's have a look"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7iTXf_eBqhF"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import Lambda\n",
        "\n",
        "def add_noise(x, alpha=0.1):\n",
        "    return x + alpha*torch.rand_like(x)\n",
        "\n",
        "\n",
        "transform = Compose([ToTensor(),\n",
        "                     ###\n",
        "                     ])\n",
        "\n",
        "train_dataset = ###\n",
        "test_dataset = ###\n",
        "\n",
        "print(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SS7XvyYuBqhF"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "idxs = torch.randint(low=0, high=len(train_dataset), size=(6,))\n",
        "fig, axs = plt.subplots(1, 6, figsize=(15, 5))\n",
        "for i, idx in enumerate((idxs)):\n",
        "    img, target = ###\n",
        "    axs[i].imshow(img[0], cmap=\"gray\") # imshow expects W x H x C format\n",
        "    axs[i].set_title(target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoO7AE2fBqhF"
      },
      "source": [
        "* For now, all we need is the simple transformation of the dataset to ``torch.Tensor`` objects to allow ``PyTorch`` handling, and the standardisation of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDSMo6vKBqhF"
      },
      "outputs": [],
      "source": [
        "print(\"min/max:\", ###)\n",
        "print(\"mean: \", ###)\n",
        "print(\"std: \", ###)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qdee4bWrBqhF"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import Normalize\n",
        "\n",
        "transform = Compose([ToTensor(), # In this transformation the data is scaled from [0, 1], see documentation\n",
        "                     Normalize(0.19176216423511505, 0.3483428359031677), # This transforms applies a Z-score normalisation\n",
        "                     ])\n",
        "\n",
        "train_dataset = ###\n",
        "test_dataset = ###\n",
        "\n",
        "print(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kIXSnhsBqhF"
      },
      "outputs": [],
      "source": [
        "print(\"min/max:\", ###)\n",
        "print(\"mean: \", ###)\n",
        "print(\"std: \", ###)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhEx7Fa2BqhF"
      },
      "source": [
        "* Why is the data not standardised?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWynZd-yBqhF"
      },
      "source": [
        "## Batch Handling with ``DataLoader``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYWuUXkYBqhF"
      },
      "source": [
        "* The process of training in Pytorch relies on batch management of a particular dataset\n",
        "\n",
        "* This is done using the [``Dataloader``](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) module, which is responsible for accessing a [``Dataset``](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) derived object and handling it according to our needs\n",
        "\n",
        "* In a way, the ``Dataloader`` and ``Dataset`` objects complement one another. The ``Dataset`` object handles **data** processing, whereas the ``Dataloader`` handles **batch** processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzL7XUBFBqhG"
      },
      "outputs": [],
      "source": [
        "batch_size = ### # The batch size\n",
        "num_workers = ### # Subprocess for loading the data\n",
        "\n",
        "train_loader = ###\n",
        "\n",
        "print(train_loader)\n",
        "print(train_loader.__dict__) # Information held by the dataloader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxO84IHjBqhG"
      },
      "source": [
        "#### \"The ``DataLoader`` object combines a dataset and a sampler, and provides an iterable over the given dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1A-zhbpLBqhG"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "train_batch_sample, train_batch_targets = ### # syntax for directly iterating over the data loader\n",
        "print(train_batch_sample.shape, train_batch_targets.shape)\n",
        "\n",
        "# Visualise batch\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "batch_grid = ###\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(batch_grid[0], cmap=\"gray\") # index because make_grid transforms grayscale images to RGB\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()\n",
        "\n",
        "print(train_batch_targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yK4ChMEEBqhG"
      },
      "source": [
        "* Fetching the next batch of the ``train_loader`` iterator returns 32 samples and 32 corresponding target labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyFexoTEBqhG"
      },
      "outputs": [],
      "source": [
        "print(\"min/max:\", ###)\n",
        "print(\"mean: \", ###)\n",
        "print(\"std: \", ###)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev91FjR6BqhG"
      },
      "source": [
        "## Training workflow: ``Dataset`` and  ``Dataloader`` in Perspective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BeiHWxbBqhG"
      },
      "source": [
        "* Let's begin by splitting our training dataset into training and validation using ``StratifiedShuffleSplit`` and ``Subset``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Myok3FEhBqhG"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "shuffler = ###\n",
        "train_idxs, valid_idxs = [(train_idx, valid_idx) for train_idx, valid_idx in shuffler][0]\n",
        "\n",
        "# Split data with Subset (does not require data to be loaded into memory, it limits the range of which __getitem__ can be called)\n",
        "from torch.utils.data import Subset\n",
        "valid_dataset = ###\n",
        "train_dataset = ###\n",
        "\n",
        "# Confirm split ratio and no overlapping idxs\n",
        "print(\"train ratio:\", ###)\n",
        "print(\"valid ratio:\", ###)\n",
        "print(\"no overlapping values\", ###)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNeCmNKiBqhG"
      },
      "outputs": [],
      "source": [
        "print(type(train_dataset))\n",
        "print(train_dataset.__dict__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62-XJb0IBqhG"
      },
      "source": [
        "* ``Subset`` is one of the many useful tools created by Pytorch to manipulate a ``Dataset`` object without creating copies. Other operations include concatenation and chaining, and can be found in the documentation [here](https://pytorch.org/docs/stable/data.html)\n",
        "\n",
        "* We now need two dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "491FnehpBqhG"
      },
      "outputs": [],
      "source": [
        "batch_size = 64 # The batch size\n",
        "num_workers = 0 # Subprocess for loading the data\n",
        "\n",
        "train_loader = ###\n",
        "\n",
        "valid_loader = ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZzWPklnBqhH"
      },
      "source": [
        "#### Let's now return to the simple feed-forward network from the previous lecture, with some custom modifications\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTg59noxBqhH"
      },
      "outputs": [],
      "source": [
        "class simpleFFN(nn.Module):\n",
        "  def __init__(self, ###):\n",
        "    super(simpleFFN, self).__init__()\n",
        "    self.hidden_1 = nn.Linear(###, ###, bias=False)\n",
        "    self.hidden_2 = nn.Linear(###, ###, bias=False)\n",
        "    self.output = nn.Linear(###, ###, bias=False)\n",
        "    self.activation = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, X):\n",
        "    z1 = self.hidden_1(X)\n",
        "    a1 = self.activation(z1)\n",
        "    z2 = self.hidden_2(a1)\n",
        "    a2 = self.activation(z2)\n",
        "    z3 = self.output(a2)\n",
        "    a3 = self.activation(z3)\n",
        "    return a3\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSVsel4nBqhH"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "model = ###\n",
        "\n",
        "# Check number of parameters\n",
        "nparam_layer1 = ###\n",
        "nparam_layer2 = ###\n",
        "nparam_layer3 = ###\n",
        "print(nparam_layer1 + nparam_layer2 + nparam_layer3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHYbC07UBqhH"
      },
      "source": [
        "* More conveniently:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXbEU2onBqhH"
      },
      "outputs": [],
      "source": [
        "sum(###)  #.parameters() is a method inherited from the nn.Module base class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwdNbE-xBqhH"
      },
      "source": [
        "* Even more conveniently:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XohN-SHKBqhH"
      },
      "outputs": [],
      "source": [
        "from torchsummary import summary\n",
        "summ ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwxCE-hVBqhH"
      },
      "outputs": [],
      "source": [
        "# Test model input and output sizes with batch sample\n",
        "x = ###\n",
        "y = model(x).to(device)\n",
        "print(x.shape, y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tyTcuvHBqhH"
      },
      "source": [
        "### The Training Workflow: Forward, Loss, Backpropagate, Optimise, Repeat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFqgSHXJBqhH"
      },
      "source": [
        "* Now that we have our data loaders ready to be iterated, let's revisit the training workflow from the last lecture\n",
        "\n",
        "* The general steps for training a model are:\n",
        "    1. Peform a foward pass of the **batch** throught the model to get initial outputs\n",
        "\n",
        "    2. Compare the outputs with the desired targets using a difference ``metric`` to obtain a ``loss`` measure\n",
        "\n",
        "    3. Compute derivatives of all model weights and biases with respect to the loss using back propagation\n",
        "    \n",
        "    4. Take an optimisation step and repeat for next batch\n",
        "\n",
        "\n",
        "* Once the network goes through all batches, an ``epoch`` is concluded\n",
        "\n",
        "\n",
        "* Let's use [``Adam``](https://arxiv.org/abs/1412.6980) as optimiser and [``Cross-Entropy``](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) as our metric\n",
        "\n",
        "\n",
        "* First we define our ``loss``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNaqbgpyBqhH"
      },
      "outputs": [],
      "source": [
        "criterion = ###\n",
        "print(criterion.__dict__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvRVml1DBqhH"
      },
      "source": [
        "* The optimiser is responsible for navigating the loss space towards the gradient direction, but it is **NOT** responsible for computing these gradients\n",
        "\n",
        "* Commonly, these gradients are calculated using the ``.backward()`` method, which makes use of automatic differention and the computational graph to perform the operation.\n",
        "\n",
        "* The gradients computed are stored within each ``torch.tensor`` that defines a trainable parameter. In our case, this refers to each model parameter. At each optimisation step, the optimiser updates the values of such tensors in their gradient direction for a distance specified by the learning rate, along with other parameters (e.g. momentum, weight decay)\n",
        "\n",
        "* When constructing an optimsier, the two important things to specify are: 1) which parameters to optimise, and 2) the learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkWmY_iqBqhH"
      },
      "outputs": [],
      "source": [
        "optimiser = ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMZQxAZxBqhI"
      },
      "outputs": [],
      "source": [
        "first_param_layer = ###\n",
        "print(first_param_layer.shape)\n",
        "print(first_param_layer)\n",
        "print(first_param_layer.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKZ0KcvUBqhI"
      },
      "source": [
        "### Training and Validation Loops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOcWE-swBqhI"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def train(model, optimizer, criterion, data_loader):\n",
        "    ###\n",
        "    train_loss, train_accuracy = 0, 0\n",
        "    for input, target in data_loader:                       # Iterate over the mini-batches defined in the data loader\n",
        "        input, target = input.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()                               # Resetting gradients so they don't accumulate for each .backward() pass\n",
        "\n",
        "        output = ###                               # Forward pass, evaluation of model\n",
        "        loss = ###                    # Compute loss\n",
        "        ###                                    # Backpropagation to calculate the gradients of every parameter\n",
        "                                                            # involved in calculating this loss. Gradients are stored within tensors\n",
        "\n",
        "\n",
        "        train_loss += loss*input.size(0)                    # Loss is averaged throughout batch, we scale back to later compute the\n",
        "                                                            # averaged loss for each epoch\n",
        "\n",
        "        pred = ###          # Transform last-layer activation to probabilities, select highest value as prediction\n",
        "\n",
        "        train_accuracy += accuracy_score(target.cpu().numpy(), pred.detach().cpu().numpy())*input.size(0)\n",
        "                                                            # Compute accuracy\n",
        "                                                            # Note that we didn't call .backward() on our accuracy\n",
        "                                                            # It is not a metric we are optimising for\n",
        "\n",
        "\n",
        "        ###                                    # Perform an optimisation step for all parameters and learning rate\n",
        "                                                            # defined in the construction of the optimiser\n",
        "\n",
        "    train_loss = train_loss / len(data_loader.dataset)      # Average loss over the whole dataset\n",
        "    train_accuracy = train_accuracy/len(data_loader.dataset)\n",
        "    return train_loss, train_accuracy\n",
        "\n",
        "\n",
        "def valid(model, criterion, data_loader):\n",
        "    \" Equivalent to the training function without any backpropagation or optimisation steps\"\n",
        "    ###\n",
        "    valid_loss, valid_accuracy = 0, 0\n",
        "    with ###:\n",
        "        for input, target in data_loader:\n",
        "            input, target = input.to(device), target.to(device)\n",
        "\n",
        "            output = model(input)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            valid_loss += loss*input.size(0)\n",
        "\n",
        "            pred = output.softmax(dim=1).max(dim=1)[1]\n",
        "\n",
        "            valid_accuracy += accuracy_score(target.cpu().numpy(), pred.detach().cpu().numpy())*input.size(0)\n",
        "\n",
        "        valid_loss = valid_loss / len(data_loader.dataset)\n",
        "        valid_accuracy = valid_accuracy/len(data_loader.dataset)\n",
        "        return valid_loss, valid_accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7I5S7DjBqhI"
      },
      "outputs": [],
      "source": [
        "from progressbar import ProgressBar\n",
        "\n",
        "nepochs = 5\n",
        "with ProgressBar(max_value=nepochs) as bar:\n",
        "    for i in range(nepochs):\n",
        "        train_loss, train_accuracy = ###\n",
        "        valid_loss, valid_accuracy = ###\n",
        "\n",
        "        log = {\"train_loss\": train_loss.item(), \"train_accuracy\": train_accuracy.item(),\n",
        "                \"valid_loss\": valid_loss.item(),  \"valid_accuracy\": valid_accuracy.item()}\n",
        "        print(log)\n",
        "\n",
        "        bar.update(i)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(first_param_layer.grad)\n",
        "print(first_param_layer.grad.shape)"
      ],
      "metadata": {
        "id": "C0Euhu1iJyZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FqlXhtRBqhI"
      },
      "source": [
        "* What does the final layer of our model look like?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKwbG-5LBqhI"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "output = model(###)\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.imshow(###, cmap=\"plasma\", vmin=0., vmax=1.)\n",
        "plt.xticks([i for i in range(output.shape[1])])\n",
        "plt.xlabel(\"Labels\")\n",
        "plt.ylabel(\"Samples\")\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZuIsk5wBqhI"
      },
      "source": [
        "* The plot above shows that the final layer of our trained model shows the level of \"certainty\" over a predicted label. Some are a clear choice, but some others cast some doubt.\n",
        "\n",
        "---\n",
        "\n",
        "Some considerations:\n",
        "\n",
        "* ``model.train`` and ``model.eval`` control some underlying behaviours of the model that require different behaviours during training and inference time. For instance, ``batch normalisation`` layers no longer are activated for batch-statistics and ``dropout`` layers are deactivated (more on theses concepts in the upcoming lectures)\n",
        "\n",
        "* ``torch.no_grad`` disables any gradient computation in ``torch.Tensors`` temporarily. Practically, this means more efficiency in inference since the computational graph of operations does not have to be retained.\n",
        "\n",
        "* ``.detach()`` returns a new tensor that is detached from the current computational graph (gradients can no longer be computed). This is particularly useful for processing the tensor with other libraries (e.g. numpy, scikit-learn, etc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_n5-iChmBqhI"
      },
      "source": [
        "## Deeper into ``Datasets``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuY6oRrBBqhI"
      },
      "source": [
        "* So far we have handled well-formated datasets that align very well with the ``torch`` library\n",
        "\n",
        "* But how can we handle datasets that we might find online, or develop ourselves, that require different formating structures?\n",
        "\n",
        "* ``ImageFolder`` is one useful base class provided by ``torchvision`` that relies on the following folder structure:\n",
        "\n",
        "```\n",
        "root/class_x/xxx.png\n",
        "root/class_x/xxy.png\n",
        "root/class_x/[...]/xxz.png\n",
        "\n",
        "root/class_y/123.png\n",
        "root/class_y/nsdf3.png\n",
        "root/class_y/[...]/asd932_.png\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RX8kDvM4BqhI"
      },
      "outputs": [],
      "source": [
        "%%html\n",
        "<iframe src=\"https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html\" width=\"600\" height=\"700\"></iframe>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ky9Lj-a0GCK8"
      },
      "source": [
        "* We will use a new dataset that is available on the [Kaggle](http://www.kaggle.com) platform\n",
        "\n",
        "* To run the following code you will need a Kaggle account and an authentication ``json`` file\n",
        "\n",
        "* To do so follow the 'Authentication' instructions on this link https://www.kaggle.com/docs/api\n",
        "\n",
        "* Once you downloaded the ``kaggle.json`` file, upload it to ``MyDrive``\n",
        "\n",
        "* You can use the snippet below to retrieve datasets from Kaggle in other projects as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDqpij32BqhI"
      },
      "outputs": [],
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle\n",
        "\n",
        "!kaggle datasets download -d gpiosenka/butterfly-images40-species\n",
        "!mkdir ./butterfly-images40-species\n",
        "!unzip -q butterfly-images40-species.zip -d ./butterfly-images40-species"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcpCE4c0BqhI"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "butterfly_train_dataset = ###\n",
        "\n",
        "print(butterfly_train_dataset, \"\\n\")\n",
        "print(butterfly_train_dataset.class_to_idx)\n",
        "\n",
        "butterfly, target = butterfly_train_dataset[42]\n",
        "plt.imshow(butterfly)\n",
        "plt.title(list(butterfly_train_dataset.class_to_idx)[target])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SpC6YrFBqhJ"
      },
      "source": [
        "* But we can also create our own dataset class to adapt to any data structure that we'd like.\n",
        "\n",
        "* We can do so by taking advantage of the ``Dataset`` class in ``PyTorch``, which can be used as an base class for custom datasets\n",
        "\n",
        "* From the documentation:\n",
        "\n",
        "```\n",
        "torch.utils.data.Dataset is an abstract class representing a dataset. Your custom dataset should inherit Dataset and override the following methods:\n",
        "\n",
        "__len__ so that len(dataset) returns the size of the dataset.\n",
        "\n",
        "__getitem__ to support the indexing such that dataset[i] can be used to get iith sample.\n",
        "\n",
        "```\n",
        "\n",
        "* Both methods are used by the ``Dataloader`` to sample the dataset. ``__getitem__`` returns a sample, and ``__len__`` defines the range in which ``__getitem__`` can be called.\n",
        "\n",
        "---\n",
        "\n",
        "* Let's now consider the same buttfly dataset, but instead of a classifiction problem, we are interested in a reconstruction one: deblurring.\n",
        "\n",
        "* We design our problem such that our input is a blurred version of our target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSCsGquLBqhJ"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import os, glob\n",
        "from PIL import Image\n",
        "from torchvision.transforms import GaussianBlur\n",
        "\n",
        "###\n",
        "\n",
        "  # def __str__(self):\n",
        "  #     class_string = \"\"\n",
        "  #     class_string += self.__class__.__name__\n",
        "  #     class_string+=\"\\n\\tlen : %d\"%self.__len__()\n",
        "  #     for key, value in self.__dict__.items():\n",
        "  #         if key != \"data_paths\":\n",
        "  #             class_string+=\"\\n\\t\" + str(key) + \" : \" + str(value)\n",
        "  #     return class_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlH44JCwBqhJ"
      },
      "outputs": [],
      "source": [
        "butterfly_train_dataset = ###\n",
        "print(butterfly_train_dataset)\n",
        "input, target = ###\n",
        "###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8kMoJRcBqhJ"
      },
      "source": [
        "## Further considerations\n",
        "\n",
        "* The options for customisation in ``Pytorch`` library are extensive. Mostly, you will **not** need to fully understand these concepts for a successful completion of this course.\n",
        "\n",
        "* The documentation of ``Pytorch`` is full of examples, tutorials and paper references and will be your best reference to learn new application concepts and how to implement them.\n",
        "\n",
        "* A few other considerations before we wrap up:\n",
        "\n",
        "Mathematical Utils in ``Pytorch``\n",
        "\n",
        "---\n",
        "[``torch.linalg``](https://pytorch.org/docs/stable/linalg.html)\n",
        "    - [``torch.fft``](https://pytorch.org/docs/stable/fft.html)\n",
        "    - [``torch.random``](https://pytorch.org/docs/stable/random.html)\n",
        "    - [``torch.sparse``](https://pytorch.org/docs/stable/sparse.html)\n",
        "    - ...\n",
        "\n",
        "\n",
        "Losses and Activations\n",
        "\n",
        "---\n",
        "\n",
        "The loss function drives the optimisation of machine learning models. It tells the model what it is meant to learn. In this sense, different tasks require different loss functions. In our examples so far, we have used ``CrossEntropy`` as our loss, which is well suited to multi-taks classification problems. In contrast, this loss function would not be suited, for example, to reconstruction problems. In this case we would require a loss function that would compare an image output to a target output. The ``MSELoss`` would be more suitable for this scenario. A wide range of loss functions are provided by ``Pytorch``, but it is up to you to decide which is more appropriate to your problem.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSzskbBCIDEs"
      },
      "outputs": [],
      "source": [
        "%%html\n",
        "<iframe src=\"https://pytorch.org/docs/stable/nn.html#loss-functions\" width=\"700\" height=\"500\"></iframe>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYQeCjWsBqhJ"
      },
      "source": [
        "Learning Rate Annealing\n",
        "\n",
        "---\n",
        "\n",
        "Learning rate annealing refers to decreasing the learning rate during the course of optimisation. This has been shown to help with convergence and stabilisation of your model, while also cutting training time. In ``Pytorch`` the implementation of learning rate annealing follows a similar syntax to the implementation of the optimiser, with different annealing strategies offered.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5dV3FCyH87v"
      },
      "outputs": [],
      "source": [
        "%%html\n",
        "<iframe src=\"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\" width=\"700\" height=\"500\"></iframe>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJEY4_vCHtN3"
      },
      "source": [
        "Dataloader sampler\n",
        "\n",
        "---\n",
        "\n",
        "As with custom ``Dataset`` classes, we can also design better sampling strategies that are better suited to our dataset. This is an interesting way of adding or removing biases in our model. For instance, using a standard sampler for a dataset that has different amounts of data per class will bias our model to perform better to the classes that contain more samples. We can instead use a ``WeightedSampler`` to increase the probability of a weaker class to show up in our batches. As per usual, ``Pytorch`` offers a wide range of sampling strategies that can be passed into the ``Dataloader``, including a base ``Sampler`` class so we can customise our own.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "remz6yimH4Ul"
      },
      "outputs": [],
      "source": [
        "%%html\n",
        "\n",
        "<iframe src=\"https://pytorch.org/docs/stable/data.html#torch.utils.data.Sampler\" width=\"700\" height=\"500\"></iframe>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0INKxBn7Hu1i"
      },
      "source": [
        "Model Initialisations\n",
        "\n",
        "---\n",
        "\n",
        "The initialisation of weights in our model can have significant impact on its convergence and performance. It is natural that ``Pytorch`` will also offer different ways to initialise the model weights. For most layers, the default initialisation method is the ``Kaiming Uniform``, which is described in the documentation:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqvik-txHseC"
      },
      "outputs": [],
      "source": [
        "%%html\n",
        "<iframe src=\"https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_uniform_\" width=\"700\" height=\"500\"></iframe>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WttozpAEPvvE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Slideshow",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "b996dd704eec390d394e4eff83c8c73a7c6e40d054c583352c9aa3265aab441b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}